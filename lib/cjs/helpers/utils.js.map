{"version":3,"sources":["../../../src/helpers/utils.ts"],"names":["computeChecksumMd5","file","Promise","resolve","reject","chunkSize","spark","SparkMD5","ArrayBuffer","fileReader","FileReader","cursor","onerror","processChunk","chunk_start","chunk_end","Math","min","size","readAsArrayBuffer","slice","onload","e","append","target","result","end","parseStream","stream","chunks","on","chunk","push","Buffer","from","err","JSON","parse","concat","toString","getQueryResult","source","property","fetch","single","myEngine","q","query","sources","then","i","bindings","get","value","map"],"mappings":";;;;;;;;;;;AAAA;;AACA;;;;;;;;;;AAEO,SAASA,kBAAT,CAA4BC,IAA5B,EAAyD;AAC5D,SAAO,IAAIC,OAAJ,CAAY,UAACC,OAAD,EAAUC,MAAV,EAAqB;AACtC,QAAMC,SAAS,GAAG,OAAlB,CADsC,CACX;;AAC3B,QAAMC,KAAK,GAAG,IAAIC,QAAQ,CAACC,WAAb,EAAd;AACA,QAAMC,UAAU,GAAG,IAAIC,UAAJ,EAAnB;AAEA,QAAIC,MAAM,GAAG,CAAb,CALsC,CAKtB;;AAEhBF,IAAAA,UAAU,CAACG,OAAX,GAAqB,YAAiB;AACpCR,MAAAA,MAAM,CAAC,iDAAD,CAAN;AACD,KAFD,CAPsC,CAWtC;;;AACA,aAASS,YAAT,CAAsBC,WAAtB,EAAiD;AAC/C,UAAMC,SAAS,GAAGC,IAAI,CAACC,GAAL,CAAShB,IAAI,CAACiB,IAAd,EAAoBJ,WAAW,GAAGT,SAAlC,CAAlB;AAEAI,MAAAA,UAAU,CAACU,iBAAX,CAA6BlB,IAAI,CAACmB,KAAL,CAAWN,WAAX,EAAwBC,SAAxB,CAA7B;AACD,KAhBqC,CAkBtC;AACA;AACA;AACA;;;AACAN,IAAAA,UAAU,CAACY,MAAX,GAAoB,UAASC,CAAT,EAAuB;AACzChB,MAAAA,KAAK,CAACiB,MAAN,CAAaD,CAAC,CAACE,MAAF,CAASC,MAAtB,EADyC,CACV;;AAC/Bd,MAAAA,MAAM,IAAIN,SAAV,CAFyC,CAEpB;;AAErB,UAAIM,MAAM,GAAGV,IAAI,CAACiB,IAAlB,EAAwB;AACtB;AACAL,QAAAA,YAAY,CAACF,MAAD,CAAZ;AACD,OAHD,MAGO;AACL;AACA;AACA;AACA;AAEA;AACA;AACAR,QAAAA,OAAO,CAACG,KAAK,CAACoB,GAAN,EAAD,CAAP;AACD;AACF,KAjBD;;AAmBAb,IAAAA,YAAY,CAAC,CAAD,CAAZ;AACD,GA1CM,CAAP;AA2CD;;AAEI,SAASc,WAAT,CAAqBC,MAArB,EAA6B;AAClC,MAAMC,MAAM,GAAG,EAAf;AACA,SAAO,IAAI3B,OAAJ,CAAY,UAACC,OAAD,EAAUC,MAAV,EAAqB;AACtCwB,IAAAA,MAAM,CAACE,EAAP,CAAU,MAAV,EAAkB,UAACC,KAAD;AAAA,aAAWF,MAAM,CAACG,IAAP,CAAYC,MAAM,CAACC,IAAP,CAAYH,KAAZ,CAAZ,CAAX;AAAA,KAAlB;AACAH,IAAAA,MAAM,CAACE,EAAP,CAAU,OAAV,EAAmB,UAACK,GAAD;AAAA,aAAS/B,MAAM,CAAC+B,GAAD,CAAf;AAAA,KAAnB;AACAP,IAAAA,MAAM,CAACE,EAAP,CAAU,KAAV,EAAiB,YAAK;AACpB3B,MAAAA,OAAO,CAACiC,IAAI,CAACC,KAAL,CAAWJ,MAAM,CAACK,MAAP,CAAcT,MAAd,EAAsBU,QAAtB,CAA+B,MAA/B,CAAX,CAAD,CAAP;AACD,KAFD;AAGD,GANM,CAAP;AAOD;;SAEqBC,c;;;;;4EAAf,iBAA8BC,MAA9B,EAAsCC,QAAtC,EAAgDC,KAAhD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAuDC,YAAAA,MAAvD,2DAAyE,KAAzE;AACCC,YAAAA,QADD,GACY,iCADZ;AAECC,YAAAA,CAFD,iCAE4BL,MAF5B,gBAEwCC,QAFxC;AAAA;AAAA,mBAGkBG,QAAQ,CAACE,KAAT,CAAeD,CAAf,EAAkB;AAACE,cAAAA,OAAO,EAAE,CAACP,MAAD,CAAV;AAAoBE,cAAAA,KAAK,EAALA;AAApB,aAAlB,EAA8CM,IAA9C,CAAmD,UAACC,CAAD;AAAA,qBAA6BA,CAAC,CAACC,QAAF,EAA7B;AAAA,aAAnD,CAHlB;;AAAA;AAGCA,YAAAA,QAHD;;AAAA,iBAIDP,MAJC;AAAA;AAAA;AAAA;;AAAA,6CAKIO,QAAQ,CAAC,CAAD,CAAR,CAAYC,GAAZ,CAAgB,MAAhB,EAAwBC,KAL5B;;AAAA;AAAA,6CAOIF,QAAQ,CAACG,GAAT,CAAa,UAAAJ,CAAC;AAAA,qBAAIA,CAAC,CAACE,GAAF,CAAM,MAAN,EAAcC,KAAlB;AAAA,aAAd,CAPJ;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G","sourcesContent":["import * as SparkMD5 from 'spark-md5';\nimport { IQueryResultBindings, newEngine } from '@comunica/actor-init-sparql';\n\nexport function computeChecksumMd5(file: File): Promise<string> {\n    return new Promise((resolve, reject) => {\n      const chunkSize = 5242880; // Read in chunks of 5MB\n      const spark = new SparkMD5.ArrayBuffer();\n      const fileReader = new FileReader();\n  \n      let cursor = 0; // current cursor in file\n  \n      fileReader.onerror = function(): void {\n        reject('MD5 computation failed - error reading the file');\n      };\n  \n      // read chunk starting at `cursor` into memory\n      function processChunk(chunk_start: number): void {\n        const chunk_end = Math.min(file.size, chunk_start + chunkSize);\n        \n        fileReader.readAsArrayBuffer(file.slice(chunk_start, chunk_end));\n      }\n  \n      // when it's available in memory, process it\n      // If using TS >= 3.6, you can use `FileReaderProgressEvent` type instead \n      // of `any` for `e` variable, otherwise stick with `any`\n      // See https://github.com/Microsoft/TypeScript/issues/25510\n      fileReader.onload = function(e: any): void {\n        spark.append(e.target.result); // Accumulate chunk to md5 computation\n        cursor += chunkSize; // Move past this chunk\n  \n        if (cursor < file.size) {\n          // Enqueue next chunk to be accumulated\n          processChunk(cursor);\n        } else {\n          // Computation ended, last chunk has been processed. Return as Promise value.\n          // This returns the base64 encoded md5 hash, which is what\n          // Rails ActiveStorage or cloud services expect\n          // resolve(btoa(spark.end(true)));\n  \n          // If you prefer the hexdigest form (looking like\n          // '7cf530335b8547945f1a48880bc421b2'), replace the above line with:\n          resolve(spark.end());\n        }\n      };\n  \n      processChunk(0);\n    });\n  }\n\nexport function parseStream(stream) {\n  const chunks = [];\n  return new Promise((resolve, reject) => {\n    stream.on(\"data\", (chunk) => chunks.push(Buffer.from(chunk)));\n    stream.on(\"error\", (err) => reject(err));\n    stream.on(\"end\", () =>{\n      resolve(JSON.parse(Buffer.concat(chunks).toString(\"utf8\")))\n    });\n  });\n}\n\nexport async function getQueryResult(source, property, fetch, single: boolean = false) {\n  const myEngine = newEngine()\n  const q = `SELECT ?res WHERE {<${source}> <${property}> ?res}`\n  const bindings = await myEngine.query(q, {sources: [source], fetch}).then((i: IQueryResultBindings) => i.bindings())\n  if (single) {\n    return bindings[0].get(\"?res\").value\n  } else {\n    return bindings.map(i => i.get(\"?res\").value)\n  }\n}"],"file":"utils.js"}